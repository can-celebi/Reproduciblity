setwd("~/Desktop/Reproduciblity/text")
library(tidyverse)
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
install.packages("tidyverse")
# sudo apt update
# sudo apt install libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev
# install.packages("tidyverse")
library(tidyverse)
setwd("~/Desktop/Reproduciblity/text")
files <- list(
p1 = "p1.csv",
p2 = "p2.csv",
l1 = "l1.csv",
l2 = "l2.csv",
l3 = "l3.csv"
)
diagnose_csv <- function(name, path) {
cat("\n")
cat("================================================================\n")
cat(paste0("DATASET: ", name, "\n"))
cat(paste0("FILE: ", path, "\n"))
cat("================================================================\n")
if (!file.exists(path)) {
cat("ERROR: File not found!\n")
return(NULL)
}
df <- read_csv(path, show_col_types = FALSE)
cat(paste0("\nDimensions: ", nrow(df), " rows × ", ncol(df), " columns\n"))
cat("\n--- COLUMN NAMES ---\n")
cat(paste(colnames(df), collapse = ", "), "\n")
cat("\n--- COLUMN TYPES ---\n")
col_info <- tibble(
column = colnames(df),
type = sapply(df, function(x) class(x)[1]),
n_unique = sapply(df, n_distinct),
n_na = sapply(df, function(x) sum(is.na(x))),
example = sapply(df, function(x) {
val <- na.omit(x)[1]
if (is.character(val)) {
substr(val, 1, 50)
} else {
as.character(val)
}
})
)
print(col_info, n = 50)
# Find likely text columns (character columns with long content)
cat("\n--- LIKELY TEXT COLUMNS (for 'input') ---\n")
text_cols <- df %>%
select(where(is.character)) %>%
summarise(across(everything(), ~mean(nchar(., na.rm = TRUE), na.rm = TRUE))) %>%
pivot_longer(everything(), names_to = "column", values_to = "avg_length") %>%
arrange(desc(avg_length))
print(text_cols, n = 10)
# Find likely ID columns
cat("\n--- LIKELY ID COLUMNS (for 'localId') ---\n")
id_cols <- col_info %>%
filter(n_unique == nrow(df) | n_unique > nrow(df) * 0.9) %>%
select(column, n_unique)
if (nrow(id_cols) > 0) {
print(id_cols)
} else {
cat("No columns with unique values found. May need composite key.\n")
# Show columns with high uniqueness
col_info %>%
arrange(desc(n_unique)) %>%
head(5) %>%
select(column, n_unique) %>%
print()
}
# Find categorical columns (potential ground truth)
cat("\n--- CATEGORICAL COLUMNS (potential ground truth) ---\n")
cat_cols <- col_info %>%
filter(n_unique <= 20 & n_unique > 1) %>%
select(column, n_unique)
print(cat_cols, n = 20)
# Show unique values for small categorical columns
cat("\n--- CATEGORY VALUES ---\n")
for (col in cat_cols$column) {
if (col_info$n_unique[col_info$column == col] <= 10) {
cat(paste0(col, ": ", paste(sort(unique(df[[col]])), collapse = ", "), "\n"))
}
}
# First 3 rows
cat("\n--- FIRST 3 ROWS ---\n")
print(head(df, 3))
return(df)
}
cat("\n\n")
cat("########################################################################\n")
cat("# CSV DIAGNOSTIC REPORT                                                #\n")
cat("# Copy everything below this line and share with Claude               #\n")
cat("########################################################################\n")
datasets <- list()
for (name in names(files)) {
datasets[[name]] <- diagnose_csv(name, files[[name]])
}
View(datasets)
# =============================================================================
# CSV Diagnostic Script for LLM Classification Study
# Run this in RStudio and share the console output
# =============================================================================
# sudo apt update
# sudo apt install libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev
# install.packages("tidyverse")
library(tidyverse)
setwd("~/Desktop/Reproduciblity/text")
# =============================================================================
# EDIT THESE PATHS TO YOUR CSV FILES
# =============================================================================
files <- list(
p1 = "p1.csv",# =============================================================================
# CSV Diagnostic Script for LLM Classification Study - FIXED
# =============================================================================
# sudo apt update
# sudo apt install libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev
# install.packages("tidyverse")
library(tidyverse)
setwd("~/Desktop/Reproduciblity/text")
# sudo apt update
# sudo apt install libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev
# install.packages("tidyverse")
library(tidyverse)
setwd("~/Desktop/Reproduciblity/text")
files <- list(
p1 = "p1.csv",# =============================================================================
# CSV Diagnostic Script for LLM Classification Study - FIXED
# =============================================================================
# sudo apt update
# sudo apt install libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev
# install.packages("tidyverse")
library(tidyverse)
setwd("~/Desktop/Reproduciblity/text")
# =============================================================================
# CSV Diagnostic Script for LLM Classification Study - FIXED
# =============================================================================
# sudo apt update
# sudo apt install libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev
# install.packages("tidyverse")
library(tidyverse)
setwd("~/Desktop/Reproduciblity/text")
files <- list(
p1 = "p1.csv",
p2 = "p2.csv",
l1 = "l1.csv",
l2 = "l2.csv",
l3 = "l3.csv"
)
diagnose_csv <- function(name, path) {
cat("\n")
cat("================================================================\n")
cat(paste0("DATASET: ", name, "\n"))
cat(paste0("FILE: ", path, "\n"))
cat("================================================================\n")
if (!file.exists(path)) {
cat("ERROR: File not found!\n")
return(NULL)
}
df <- read_csv(path, show_col_types = FALSE)
cat(paste0("\nDimensions: ", nrow(df), " rows × ", ncol(df), " columns\n"))
cat("\n--- COLUMN NAMES ---\n")
cat(paste(colnames(df), collapse = ", "), "\n")
cat("\n--- COLUMN TYPES ---\n")
col_info <- tibble(
column = colnames(df),
type = sapply(df, function(x) class(x)[1]),
n_unique = sapply(df, n_distinct),
n_na = sapply(df, function(x) sum(is.na(x))),
example = sapply(df, function(x) {
val <- na.omit(x)[1]
if (is.character(val)) {
substr(val, 1, 50)
} else {
as.character(val)
}
})
)
print(col_info, n = 50)
# Find likely text columns (character columns with long content)
cat("\n--- LIKELY TEXT COLUMNS (for 'input') ---\n")
char_cols <- df %>% select(where(is.character))
if (ncol(char_cols) > 0) {
avg_lengths <- sapply(char_cols, function(x) mean(nchar(x[!is.na(x)])))
text_cols <- tibble(column = names(avg_lengths), avg_length = avg_lengths) %>%
arrange(desc(avg_length))
print(text_cols, n = 10)
}
# Find likely ID columns
cat("\n--- LIKELY ID COLUMNS (for 'localId') ---\n")
id_cols <- col_info %>%
filter(n_unique == nrow(df) | n_unique > nrow(df) * 0.9) %>%
select(column, n_unique)
if (nrow(id_cols) > 0) {
print(id_cols)
} else {
cat("No columns with unique values found.\n")
col_info %>%
arrange(desc(n_unique)) %>%
head(5) %>%
select(column, n_unique) %>%
print()
}
# Find categorical columns (potential ground truth)
cat("\n--- CATEGORICAL COLUMNS (2-20 unique values) ---\n")
cat_cols <- col_info %>%
filter(n_unique <= 20 & n_unique >= 2) %>%
select(column, n_unique)
print(cat_cols, n = 20)
# Show unique values for small categorical columns
cat("\n--- CATEGORY VALUES ---\n")
for (col in cat_cols$column) {
n_uniq <- col_info$n_unique[col_info$column == col]
if (n_uniq <= 10) {
vals <- sort(unique(df[[col]]))
cat(paste0(col, " (", n_uniq, "): ", paste(vals, collapse = ", "), "\n"))
}
}
# First 2 rows transposed for readability
cat("\n--- SAMPLE ROW ---\n")
sample_row <- df[1, ] %>%
pivot_longer(everything(), names_to = "column", values_to = "value") %>%
mutate(value = as.character(value),
value = ifelse(nchar(value) > 80, paste0(substr(value, 1, 80), "..."), value))
print(sample_row, n = 50)
return(df)
}
cat("\n########################################################################\n")
cat("# CSV DIAGNOSTIC REPORT - Copy output below                            #\n")
cat("########################################################################\n")
datasets <- list()
for (name in names(files)) {
datasets[[name]] <- diagnose_csv(name, files[[name]])
}
library(tidyverse)
files <- list(
p1 = "p1.csv",
p2 = "p2.csv",
l1 = "l1.csv",
l2 = "l2.csv",
l3 = "l3.csv"
)
diagnose_csv <- function(name, path) {
cat("\n================================================================\n")
cat(paste0("DATASET: ", name, " | FILE: ", path, "\n"))
cat("================================================================\n")
if (!file.exists(path)) { cat("FILE NOT FOUND!\n"); return(NULL) }
df <- read_csv(path, show_col_types = FALSE)
cat(paste0("Dimensions: ", nrow(df), " rows × ", ncol(df), " cols\n\n"))
cat("COLUMNS:\n")
for (col in colnames(df)) {
n_uniq <- n_distinct(df[[col]])
n_na <- sum(is.na(df[[col]]))
type <- class(df[[col]])[1]
# Show unique values if <= 8
if (n_uniq <= 8) {
vals <- paste(head(sort(unique(df[[col]])), 8), collapse = ", ")
cat(sprintf("  %-30s %s (n=%d, na=%d): %s\n", col, type, n_uniq, n_na, vals))
} else {
cat(sprintf("  %-30s %s (n=%d, na=%d)\n", col, type, n_uniq, n_na))
}
}
# Show text column lengths
cat("\nTEXT COLUMN LENGTHS:\n")
char_cols <- df %>% select(where(is.character))
for (col in colnames(char_cols)) {
avg_len <- mean(nchar(char_cols[[col]][!is.na(char_cols[[col]])]))
if (avg_len > 20) {
cat(sprintf("  %-30s avg %.0f chars\n", col, avg_len))
}
}
return(df)
}
# Run
cat("\n######## DIAGNOSTIC REPORT ########\n")
datasets <- list()
for (name in names(files)) {
datasets[[name]] <- diagnose_csv(name, files[[name]])
}
# L2
l2 <- read_csv("l2.csv", show_col_types = FALSE)
cat("l2:", nrow(l2), "rows\n")
cat("Columns:", paste(colnames(l2), collapse = ", "), "\n")
# L3
l3 <- read_csv("l3.csv", show_col_types = FALSE)
cat("l3:", nrow(l3), "rows\n")
cat("Columns:", paste(colnames(l3), collapse = ", "), "\n")
library(tidyverse)
p1 <- read_csv("p1.csv", show_col_types = FALSE)
p2 <- read_csv("p2.csv", show_col_types = FALSE)
l1 <- read_csv("l1.csv", show_col_types = FALSE)
l2 <- read_csv("l2.csv", show_col_types = FALSE)
l3 <- read_csv("l3.csv", show_col_types = FALSE)
# P1: promise classification (38 rows)
p1_instances <- p1 %>%
transmute(
localId = messageId,
dataset = "p1",
task = "promise",
input = Message
)
cat("p1:", nrow(p1_instances), "instances\n")
# P2: promise classification (719 rows)
p2_instances <- p2 %>%
transmute(
localId = as.character(teamID),
dataset = "p2",
task = "promise",
input = CombinedChat
)
cat("p2:", nrow(p2_instances), "instances\n")
# L1: level classification (493 rows)
l1_instances <- l1 %>%
transmute(
localId = id,
dataset = "l1",
task = "level",
input = message
)
cat("l1:", nrow(l1_instances), "instances\n")
# L2: TWO tasks - level AND belief (851 rows × 2 = 1702 instances)
l2_level <- l2 %>%
transmute(
localId = paste(subject.id, message.id, sep = "_"),
dataset = "l2",
task = "level",
input = input
)
l2_belief <- l2 %>%
transmute(
localId = paste(subject.id, message.id, sep = "_"),
dataset = "l2",
task = "belief",
input = input
)
cat("l2_level:", nrow(l2_level), "instances\n")
cat("l2_belief:", nrow(l2_belief), "instances\n")
# L3: TWO tasks - level AND belief (78 rows × 2 = 156 instances)
l3_level <- l3 %>%
transmute(
localId = as.character(message.id),
dataset = "l3",
task = "level",
input = message
)
l3_belief <- l3 %>%
transmute(
localId = as.character(message.id),
dataset = "l3",
task = "belief",
input = message
)
cat("l3_level:", nrow(l3_level), "instances\n")
cat("l3_belief:", nrow(l3_belief), "instances\n")
all_instances <- bind_rows(
p1_instances,
p2_instances,
l1_instances,
l2_level,
l2_belief,
l3_level,
l3_belief
) %>%
mutate(id = row_number()) %>%
select(id, localId, dataset, task, input)
cat("\n========================================\n")
cat("TOTAL INSTANCES:", nrow(all_instances), "\n")
cat("========================================\n\n")
cat("Breakdown:\n")
all_instances %>%
count(dataset, task) %>%
print()
# Check for any NA inputs
na_count <- sum(is.na(all_instances$input))
cat("\nNA inputs:", na_count, "\n")
# Check for empty inputs
empty_count <- sum(all_instances$input == "" | is.na(all_instances$input))
cat("Empty inputs:", empty_count, "\n")
write_csv(all_instances, "instances.csv")
cat("\nSaved to instances.csv\n")
# Preview
cat("\nFirst 10 rows:\n")
print(head(all_instances, 10))
cat("\nLast 10 rows:\n")
print(tail(all_instances, 10))
View(all_instances)
# =============================================================================
# CREATE TEST SUBSET (add this to the end of build_instances.R)
# =============================================================================
# Sample ~10 instances from each dataset_task combination
set.seed(42)  # For reproducibility
test_instances <- all_instances %>%
group_by(dataset, task) %>%
slice_sample(n = 10) %>%  # 10 from each group
ungroup() %>%
mutate(id = row_number()) %>%  # Re-number IDs
select(id, localId, dataset, task, input)
cat("\n========================================\n")
cat("TEST SUBSET\n")
cat("========================================\n\n")
cat("Total test instances:", nrow(test_instances), "\n\n")
test_instances %>%
count(dataset, task) %>%
print()
# Save test file
write_csv(test_instances, "instances_test.csv")
cat("\nSaved to instances_test.csv\n")
# Preview
cat("\nPreview:\n")
test_instances %>%
group_by(dataset, task) %>%
slice_head(n = 2) %>%
select(id, localId, dataset, task, input = input) %>%
mutate(input = substr(input, 1, 50)) %>%
print(n = 20)
View(test_instances)
